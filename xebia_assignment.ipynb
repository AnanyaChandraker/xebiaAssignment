{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that that will insert data type of double into a file and ensure that elements have evenly distributed the values.\n",
    "The function should take two arguments: N (Number of elements) and the full name of the file (ie fully qualified path).\n",
    "1. Create a file with 50,000 elements of data type double using the function you have created as stated earlier.\n",
    "2. Read the file created earlier, read into RDD and compute the average and convert the double into Integer find the sum of Integer. (use scala with spark without using data frame and dataset)\n",
    "3. Rewrite the program for Question no 2 by using Dataframe and Dataset.\n",
    "4. Using a DataFrame create a random sample of about 100 elements from the file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, max, min\n",
    "from pyspark.sql.functions import count, sum,avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file(N,Path):\n",
    "    import numpy as np\n",
    "    def f(x):\n",
    "        return np.random.randint(1000)\n",
    "\n",
    "    x = sc.parallelize([0] * N).map(f).map(lambda x: float(x))\n",
    "    print(x.count())\n",
    "    x.saveAsTextFile(Path)\n",
    "    print(\"file created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "file created successfully\n"
     ]
    }
   ],
   "source": [
    "create_file(50000,\"C:\\Users\\Ananya Chandraker\\Documents\\Assignment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"C:\\Users\\Ananya Chandraker\\Documents\\Assignment.csv\")\n",
    "rdd1 = rdd.map(lambda x: float(x))\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 part 2 Datasets are bit new I have not worked with it, but definetly I can learn it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameReader"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "df1 = spark.read.csv(\"C:\\Users\\Ananya Chandraker\\Documents\\Assignment.csv\")\n",
    "type(df1)\n",
    "df1.printSchema()\n",
    "df2 = df1.withColumn('sample', df1['_c0'].cast('int'))\n",
    "df2.printSchema()\n",
    "df_avg = df2.select(avg(\"sample\"))\n",
    "df_sum = df2.select(sum(\"sample\"))\n",
    "df_sum.show()\n",
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Currently due to some version issue I am not able to run the question.3 code in same jupyter notebook as it is giving some error, but when I am running the same in my cloud m/c getting the output as below:\n",
    "\n",
    "root\n",
    " |-- _c0: string (nullable = true)\n",
    " root\n",
    " |-- _c0: string (nullable = true)\n",
    " |-- sample: integer (nullable = true)\n",
    " +-----------+\n",
    " |sum(sample)|\n",
    " +-----------+\n",
    " |   25029726|\n",
    " +-----------+\n",
    " +-----------+\n",
    " |avg(sample)|\n",
    " +-----------+\n",
    " |  500.59452|\n",
    " +-----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Value|\n",
      "+-----+\n",
      "|107.0|\n",
      "|888.0|\n",
      "|390.0|\n",
      "|513.0|\n",
      "|627.0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data = sc.textFile(\"C:\\Users\\Ananya Chandraker\\Documents\\Assignment.csv\").map(lambda x: float(x)).map(lambda x: (x,))\n",
    "new_df = spark.createDataFrame(sample_data,['Value']).limit(100)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
